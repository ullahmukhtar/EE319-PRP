{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Some LaTeX definitions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "$$\\newcommand{\\diff}{\\mathop{}\\!\\mathrm{d}}\n",
       "\\DeclareMathOperator{\\Diff}{D}\n",
       "\\newcommand{\\euler}{\\mathrm{e}}\n",
       "\\DeclareMathOperator{\\EE}{E}\n",
       "\\DeclareMathOperator{\\Var}{Var}\n",
       "\\DeclareMathOperator{\\Cov}{Cov}\n",
       "\\DeclareMathOperator{\\Ber}{Ber}\n",
       "\\DeclareMathOperator{\\Ud}{Ud}\n",
       "\\DeclareMathOperator{\\U}{U}\n",
       "\\DeclareMathOperator{\\ind}{\\mathbf{1}}\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run nbinitex.ipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "graffitiCellId": "id_0m3re3b"
   },
   "source": [
    "# EE319 - Probability & Random Processes\n",
    "## *Dr.-Ing. Mukhtar Ullah*, FAST NUCES, Spring 2020\n",
    "<hr>\n",
    "\n",
    "## **Lecture 18** (2020-04-xx)\n",
    "## Expectation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "graffitiCellId": "id_jg2qy00"
   },
   "source": [
    "#### Typical value of a distribution\n",
    "Consider a discrete distribution with values $k\\in\\mathcal{X}^{\\#}$ and probabilities $p_{k}$. If the distribution is symmetrical about a single peak, then the most likely value, called the mode, can be considered a typical value. For distributions of other shapes, the mode may not be appropriate. Other alternatives include the median and the mean. The median $m$ of a distribution generalizes the notion of middle value and is defined as the value at which the CDF equals $1/2$. The mean $\\mu$ of a distribution is the value that best fits long-run observations in a least squared sense. We can get an estimate $\\hat{\\mu}_{n}$ of the mean based on the sum of its squared Euclidean distances from observations in $n$ trials of the underlying experiment, treated as a function of $\\hat{\\mu}_{n}$,\n",
    "$$\n",
    "S\\left(\\hat{\\mu}_{n}\\right)=\\sum_{k\\in\\mathcal{X}^{\\#}}\\left(k-\\hat{\\mu}_{n}\\right)^{2}a_{n}\\left(k\\right)\n",
    "$$\n",
    "where $a_{n}\\left(k\\right)$ is the observed frequency of $k$ in $n$ trials. Note that absolute frequencies must add up to $n$. Since $\\hat{\\mu}_{n}$ minimizes $S\\left(\\hat{\\mu}_{n}\\right)$ by definition, we require $S^{\\prime}\\left(\\hat{\\mu}_{n}\\right)=0$ which leads to\n",
    "$$\n",
    "-2\\sum_{k\\in\\mathcal{X}^{\\#}}\\left(k-\\hat{\\mu}_{n}\\right)a_{n}\\left(k\\right)=0\\implies n\\hat{\\mu}_{n}=\\sum_{k\\in\\mathcal{X}^{\\#}}ka_{n}\\left(k\\right)\\implies\\hat{\\mu}_{n}=\\frac{1}{n}\\sum_{k\\in\\mathcal{X}^{\\#}}ka_{n}\\left(k\\right)=\\sum_{k\\in\\mathcal{X}^{\\#}}kr_{n}\\left(k\\right)\n",
    "$$\n",
    "where $r_{k}\\left(n\\right)$ is the relative frequency of $k$ in $n$ trials. Since the long-run relative frequencies $r_{n}\\left(k\\right)$ approach probabilities $p_{k}$, we have the following formula for the mean of a discrete distribution\n",
    "$$\n",
    "\\mu=\\sum_{k\\in\\mathcal{X}^{\\#}}kp_{k}\n",
    "$$\n",
    "This makes sense as each value $x_{i}$ of the RV $X$ is weighted by the associated probability $p_{i}$. Owing to this probability weighting, the mean is also called the expected value, or expectation, of the RV. The expectation of a RV $X$, denoted by $\\EE X$, is defined by\n",
    "$$\n",
    "\\mu_{X}=\\EE X=\\intop_{\\mathbb{R}}xP_{X}\\left(\\diff \\mathsf{B}_{x}\\right)=\\sum_{k\\in\\mathcal{X}^{\\#}}kf^{\\#}_{X}\\left(k\\right)+\\intop_{\\mathcal{X}}xf^{\\wedge}_{X}\\left(x\\right)\\diff x\n",
    "$$\n",
    "Note that the expectation is not a point function like a transformation. Instead, the expectation is an example of what is called an operator. An operator takes a function and returns another function. A special kind of an operator, called a functional, collapses a function to a number. You can now see that the expectation operator acts on a RV, a function, and returns the mean, a number. Thus, the expectation operator is a functional. The notation $\\EE X$ without using any brackets not only reminds of the operator involved but also helps to distinguish it from a transformation $g\\left(X\\right)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "graffitiCellId": "id_ou7x9gu"
   },
   "source": [
    "#### Expectation of a transformed RV\n",
    "Consider a transformation $Y=g\\left(X\\right)$ where $g$ is a point function making any value $x$ of $X$ to the value $g\\left(x\\right)$ of $Y$. How are the two expectations $\\EE X$ and $\\EE Y$ related? Let us work that out by using the above definition of expectation and noting that the differential regions $\\diff \\mathsf{B}_{x}$ and $\\diff \\mathsf{B}_{y}$ in the Borel field correspond to the same event in the original event space. Any value $x$ of $X$ determines\n",
    "the value $y$ of $Y$ according to $y=g\\left(x\\right)$. Since both\n",
    "the RVs are defined on the same probabiliy space, the pre-image of\n",
    "any Borel set $\\mathsf{B}$ can be expressed as a disjoint union\n",
    "\\\\[\n",
    "Y^{-1}\\left[\\diff \\mathsf{B}_{y}\\right]=\\bigcup_{g\\left(x\\right)=y}X^{-1}\\left[\\diff \\mathsf{B}_{x}\\right]\n",
    "\\\\]\n",
    "The expectation of $Z$ then follows from the law of total probability:\n",
    "\\\\[\n",
    "\\EE Y=\\intop_{\\mathbb{R}}yP_{Y}\\left(\\diff \\mathsf{B}_{y}\\right)=\\intop_{\\mathbb{R}}y\\sum_{g\\left(x\\right)=y}P_{X}\\left(\\diff \\mathsf{B}_{x}\\right)=\\intop_{\\mathbb{R}}g\\left(x\\right)P_{X}\\left(\\diff \\mathsf{B}_{x}\\right)\n",
    "\\\\]\n",
    "In other words, we do not need the distribution of $Y$ to compute\n",
    "the expectation. This makes sense in terms of long-run relative frequencies.\n",
    "The observed long-run relative frequency of a value $x$ is also the\n",
    "frequency $g\\left(x\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "graffitiCellId": "id_6d97bnd"
   },
   "source": [
    "Let us work out the mean for the two RVs discussed above.\n",
    "\n",
    ">**Mean of the Dirac distribution**\n",
    ">The mean of the deterministic RV $a\\sim\\delta_{a}$ is\n",
    "$$\n",
    "\\EE a=\\sum_{k\\in\\left\\{ a\\right\\} }k\\delta_{a}\\left(\\left\\{ k\\right\\} \\right)=a\n",
    "$$\n",
    "In other words, the expected value of a (deterministic) constant is the constant itself.\n",
    "\n",
    "#### Linearity of the expectation (simpler)\n",
    "Recall from math and circuit courses that integration and summation are linear operations. You can verify from the above definition that the expectation inherits linearity from integration and summation in the following sense:\n",
    "$$\n",
    "\\EE \\left(aX+b\\right)=a\\EE X+\\EE b=a\\EE X+b\n",
    "$$\n",
    "Setting $a=1$ and $b=-\\mu_{X}$ gives \n",
    "$$\n",
    "\\EE \\left(X-\\mu_{X}\\right)=\\EE X-\\mu_{X}=0\n",
    "$$\n",
    "as expected.\n",
    "\n",
    ">**Mean of the Bernoulli distribution**\n",
    ">The mean of the Bernoulli RV $X\\sim\\Ber_{p}$ is\n",
    "$$\n",
    "\\EE X=\\sum_{k=0}^{1}k\\mathrm{Ber}\\left(k;p\\right)=\\sum_{k=0}^{1}kp^{k}\\left(1-p\\right)^{1-k}=p\n",
    "$$\n",
    "The mean of the transformed (two-point) RV $Y=a+\\left(b-a\\right)X$\n",
    "follows from linearity\n",
    "$$\n",
    "\\EE Y=a+\\left(b-a\\right)\\mathrm{E}X=a+\\left(b-a\\right)p=\\left(1-p\\right)a+pb\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Linearity of the expectation (detailed)\n",
    "A natural question arises in a curious mind here: why did we not try\n",
    "to define linearity of the expectation by this equation\n",
    "\\\\[\n",
    "\\EE\\left(aX+bY\\right)=a\\EE X+b\\EE Y\n",
    "\\\\]\n",
    "which is very reasonable to expect. To see the complication in this\n",
    "seemingly innocent equation, assume $X$ and $Y$ to be discrete,\n",
    "for convenience, and define a new RV $Z=aX+bY$ as a linear transformation.\n",
    "Any pair $\\left(x,y\\right)$ of values of $\\left(X,Y\\right)$ determines\n",
    "the value $z$ of $Z$ according to $z=ax+by$. Since all the three\n",
    "RVs are defined on the same probabiliy space, the following correspondence\n",
    "between events is evident \n",
    "\\\\[\n",
    "\\left(Z=k\\right)=\\left(aX+bY=k\\right)=\\bigcup_{ai+bj=k}\\left(X=i\\cap Y=j\\right)\n",
    "\\\\]\n",
    "The expectation of $Z$ then follows from the law of total probability:\n",
    "\\begin{align*}\n",
    "\\EE Z=\\sum_{k\\in\\mathcal{Z}}kP\\left(Z=k\\right) & =\\sum_{k\\in\\mathcal{Z}}k\\sum_{ai+bj=k}P\\left(X=i\\cap Y=j\\right)\\\\\n",
    " & =\\sum_{i\\in\\mathcal{X}}\\sum_{j\\in\\mathcal{Y}}\\left(ai+bj\\right)P_{X,Y}\\left(\\left\\{ i\\right\\} \\times\\left\\{ j\\right\\} \\right)\\\\\n",
    " & =a\\sum_{i\\in\\mathcal{X}}i\\sum_{j\\in\\mathcal{Y}}P_{X,Y}\\left(\\left\\{ i\\right\\} \\times\\left\\{ j\\right\\} \\right)+b\\sum_{j\\in\\mathcal{Y}}j\\sum_{i\\in\\mathcal{X}}P_{X,Y}\\left(\\left\\{ i\\right\\} \\times\\left\\{ j\\right\\} \\right)\\\\\n",
    " & =a\\sum_{i\\in\\mathcal{X}}iP_{X}\\left(\\left\\{ i\\right\\} \\right)+b\\sum_{j\\in\\mathcal{Y}}jP_{Y}\\left(\\left\\{ j\\right\\} \\right)\\\\\n",
    " & =a\\EE X+b\\EE Y\n",
    "\\end{align*}\n",
    "This result can be extended to general RVs. The pre-image of any Borel\n",
    "set $\\mathsf{B}$ of the $z$-axis can be expressed as a disjoint\n",
    "union\n",
    "\\\\[\n",
    "Z^{-1}\\left[\\diff \\mathsf{B}_{z}\\right]=\\bigcup_{ax+by=z}X^{-1}\\left[\\diff \\mathsf{B}_{x}\\right]\\cap Y^{-1}\\left[\\diff \\mathsf{B}_{y}\\right]\n",
    "\\\\]\n",
    "The expectation of $Z$ then follows from the law of total probability:\n",
    "\\begin{align*}\n",
    "\\EE Z=\\intop_{\\mathbb{R}}zP_{Z}\\left(\\diff \\mathsf{B}_{z}\\right) & =\\intop_{\\mathbb{R}}z\\intop_{ax+by=z}P_{X,Y}\\left(\\diff \\mathsf{B}_{x}\\times\\diff \\mathsf{B}_{y}\\right)\\\\\n",
    " & =\\intop_{\\mathbb{R}}\\intop_{\\mathbb{R}}\\left(ax+by\\right)P_{X,Y}\\left(\\diff \\mathsf{B}_{x}\\times\\diff \\mathsf{B}_{y}\\right)\\\\\n",
    " & =a\\intop_{\\mathbb{R}}x\\intop_{\\mathbb{R}}P_{X,Y}\\left(\\diff \\mathsf{B}_{x}\\times\\diff \\mathsf{B}_{y}\\right)+b\\intop_{\\mathbb{R}}y\\intop_{\\mathbb{R}}P_{X,Y}\\left(\\diff \\mathsf{B}_{x}\\times\\diff \\mathsf{B}_{y}\\right)\\\\\n",
    " & =a\\intop_{\\mathbb{R}}xP_{X}\\left(\\diff \\mathsf{B}_{x}\\right)+b\\intop_{\\mathbb{R}}yP_{Y}\\left(\\diff \\mathsf{B}_{y}\\right)\\\\\n",
    " & =a\\EE X+b\\EE Y\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "graffitiCellId": "id_pb82llw"
   },
   "source": [
    "#### Spread of a distribution\n",
    "A measure of spread of a distribution is the so-called standard deviation, denoted $\\sigma$, which can be viewed as the Euclidean distance between a typical pair of values in a distribution. The mean was introduced as a typical value in the sense of minimizing the sum of its squared Euclidean distances from long-run observations. That gives us a hint of an estimate $\\hat{\\sigma}_{n}^{2}$ of the squared standard deviation--the average squared distance per trial, \n",
    "$$\n",
    "\\hat{\\sigma}_{n}^{2}=\\frac{1}{n}S\\left(\\hat{\\mu}_{n}\\right)=\\frac{1}{n}\\sum_{k\\in\\mathcal{X}^{\\#}}\\left(k-\\hat{\\mu}_{n}\\right)^{2}a_{n}\\left(k\\right)=\\sum_{k\\in\\mathcal{X}^{\\#}}\\left(k-\\hat{\\mu}_{n}\\right)^{2}r_{n}\\left(k\\right)\n",
    "$$\n",
    "To see why $\\hat{\\sigma}_{n}$ is the Euclidean distance between a\n",
    "typical pair of values, note that both $\\left(\\hat{\\mu}_{n},\\hat{\\mu}_{n}+\\hat{\\sigma}_{n}\\right)$\n",
    "and $\\left(\\hat{\\mu}_{n},\\hat{\\mu}_{n}-\\hat{\\sigma}_{n}\\right)$ can\n",
    "be viewed as typical pairs because\n",
    "\\\\[\n",
    "\\left(\\hat{\\mu}_{n}\\pm\\hat{\\sigma}_{n}-\\hat{\\mu}_{n}\\right)^{2}=\\hat{\\sigma}_{n}^{2}\n",
    "\\\\]\n",
    "That leads us to define the standard deviation $\\sigma$ of a discrete distribution as the positive square root of what is called the variance of the distribution,\n",
    "$$\n",
    "\\sigma^{2}=\\sum_{k\\in\\mathcal{X}^{\\#}}\\left(k-\\hat{\\mu}\\right)^{2}p_{k}\n",
    "$$\n",
    "The variance of a general RV $X$, denoted $\\Var X$,\n",
    "is defined by\n",
    "$$\n",
    "\\sigma_{X}^{2}=\\Var X=\\EE \\left(X-\\mu_{X}\\right)^{2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Nonlinearity of variance (simpler)\n",
    "The squared deviation in the above formula prevents the variance operator\n",
    "to inherit linearity from integration and summation. We can study\n",
    "the nature of nonlinearity of variance by considering a linear transformation\n",
    "$Y=aX+b$ of a RV $X$ whose variance is known. Correspondence between\n",
    "values and means take the form\n",
    "\\\\[\n",
    "y=ax+b,\\quad\\mu_{Y}=a\\mu_{X}+b\n",
    "\\\\]\n",
    "This all leads to \n",
    "\\\\[\n",
    "\\Var Y=\\EE\\left(Y-\\mu_{Y}\\right)^{2}=\\EE\\left(aX+b-a\\mu_{X}-b\\right)^{2}=\\EE a^{2}\\left(X-\\mu_{X}\\right)^{2}=a^{2}\\Var X\n",
    "\\\\]\n",
    "As we expected, $\\Var\\left(aX+b\\right)\\ne a\\Var X+b$ thanks to the\n",
    "nonlinearity of variance.\n",
    "\n",
    "### Nonlinearity of variance (detailed)\n",
    "Consider a linear transformation $Z=aX+bY$ of two RVs with known\n",
    "variances. Correspondence between values and means take the form\n",
    "\\\\[\n",
    "z=ax+by,\\quad\\mu_{Z}=a\\mu_{X}+b\\mu_{Y}\n",
    "\\\\]\n",
    "This all leads to \n",
    "\\begin{align*}\n",
    "\\Var Z=\\EE\\left(Z-\\mu_{Z}\\right)^{2} & =\\EE\\left(aX+bY-a\\mu_{X}-b\\mu_{Y}\\right)^{2}\\\\\n",
    " & =a^{2}\\EE\\left(X-\\mu_{X}\\right)^{2}+b^{2}\\EE\\left(Y-\\mu_{Y}\\right)^{2}+ab\\EE\\left(X-\\mu_{X}\\right)\\left(Y-\\mu_{Y}\\right)\\\\\n",
    " & =a^{2}\\Var X+b^{2}\\Var Y+ab\\EE\\left(X-\\mu_{X}\\right)\\left(Y-\\mu_{Y}\\right)\n",
    "\\end{align*}\n",
    "As we expected, $\\Var\\left(aX+bY\\right)\\ne a\\Var X+b\\Var Y$ thanks\n",
    "to the nonlinearity of variance. The last term in the above equation\n",
    "involves what is called the covariance between two RVs:\n",
    "\\\\[\n",
    "\\Cov\\left(X,Y\\right)=\\EE\\left(X-\\mu_{X}\\right)\\left(Y-\\mu_{Y}\\right)\n",
    "\\\\]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

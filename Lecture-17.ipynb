{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Some LaTeX definitions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "$$\\newcommand{\\diff}{\\mathop{}\\!\\textnormal{d}}\n",
       "\\newcommand{\\euler}{\\mathrm{e}}\n",
       "\\DeclareMathOperator{\\PP}{P}\n",
       "\\DeclareMathOperator{\\EE}{E}\n",
       "\\DeclareMathOperator{\\Var}{Var}\n",
       "\\DeclareMathOperator{\\ind}{\\mathbf{1}}\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run nbinitex.ipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "graffitiCellId": "id_0m3re3b"
   },
   "source": [
    "# EE319 - Probability & Random Processes\n",
    "## *Dr.-Ing. Mukhtar Ullah*, FAST NUCES, Spring 2020\n",
    "<hr>\n",
    "\n",
    "## **Lecture 17** (2020-04-xx)\n",
    "## Random Variables (continued)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "graffitiCellId": "id_73grsjk"
   },
   "source": [
    "#### Bernoulli Measure (continued)\n",
    "#### Indicator function\n",
    "The definition of Bernoulli RV\n",
    "\\begin{equation}\n",
    "X\\left(\\omega\\right)=\\delta_{\\omega}\\left(\\mathsf{A}\\right)=\\left[\\omega\\in\\mathsf{A}\\right]\n",
    "\\end{equation}\n",
    "highlights two ways of viewing the Boolean expression $\\left[\\omega\\in\\mathsf{A}\\right]$.\n",
    "- Viewed as a set function of $\\mathsf{A}$, it is a Dirac measure.\n",
    "- Viewed as a point function of $\\omega$, it is a Bernoulli RV.\n",
    "\n",
    "The point-function view leads to the definition of what is called an indicator function\n",
    "$$\n",
    "\\mathbf{1}_{\\mathsf{A}}\\left(\\omega\\right)=\\delta_{\\omega}\\left(\\mathsf{A}\\right)=\\left[\\omega\\in\\mathsf{A}\\right]\n",
    "$$\n",
    "Thus, every Bernoulli RV is an indicator function. Had we known this before, our definitions of Bernoulli and deterministic RVs would have been very compact.\n",
    "- Bernoulli RV $X\\sim\\mathrm{Ber}_{p}$ would have been defined as $X=\\mathbf{1}_{\\mathsf{A}}$\n",
    "- Deterministic RV $X\\sim\\delta_{a}$ would have been defined as $X=a\\mathbf{1}_{\\mathsf{\\Omega}}$\n",
    "\n",
    "\n",
    "#### From the Bernoulli family to the two-point family\n",
    "The Bernoulli RV $X\\sim\\mathrm{Ber}_{p}$ assigns the value $1$ to (the success) $\\mathsf{A}$ and $0$ to (the failure) $\\mathsf{A^{c}}$. In a more general setting, the set $\\left\\{ 0,1\\right\\} $ could be replaced with $\\left\\{ a,b\\right\\} $. For convenience we will assume $b>a$. The resulting RV $Y$ can be defined with respect to the same event space $\\mathcal{F}$ by\n",
    "$$\n",
    "Y\\left(\\omega\\right)=a\\delta_{\\omega}\\left(\\mathsf{A^{c}}\\right)+b\\delta_{\\omega}\\left(\\mathsf{A}\\right)=a\\mathbf{1}_{\\mathsf{A^{c}}}\\left(\\omega\\right)+b\\mathbf{1}_{\\mathsf{A}}\\left(\\omega\\right)\n",
    "$$\n",
    "or, equivalently,\n",
    "$$\n",
    "Y\\left[\\mathsf{A^{c}}\\right]=\\left\\{ a\\right\\} ,\\ X\\left[\\mathsf{A}\\right]=\\left\\{ b\\right\\} \n",
    "$$\n",
    "It must be possible to derive $Y$ from $X$. How? You need not look further than the above definition. Note that the indicator function must obey\n",
    "$$\n",
    "\\mathbf{1}_{\\mathsf{A}}\\left(\\omega\\right)+\\mathbf{1}_{\\mathsf{A^{c}}}\\left(\\omega\\right)=1,\\quad\\omega\\in\\mathsf{A}\\cup\\mathsf{A^{c}}\n",
    "$$\n",
    "since $\\omega$ cannot belong to both $\\mathsf{A}$ and $\\mathsf{A^{c}}$. That simplifies the definition of $Y$ to\n",
    "$$\n",
    "Y\\left(\\omega\\right)=a+\\left(b-a\\right)\\mathbf{1}_{\\mathsf{A}}\\left(\\omega\\right)=a+\\left(b-a\\right)X\\left(\\omega\\right)\n",
    "$$\n",
    "Consequently a pair $\\left(x,y\\right)$ of values assigned by the pair $\\left(X,Y\\right)$ of RVs to any outcome $\\omega$ cannot be arbitrary and must a point in the line represented by the equation\n",
    "$$\n",
    "y=a+\\left(b-a\\right)x\n",
    "$$\n",
    "This leads to a new concept: a transformation of a RV. A transformation of a RV $X$ is a point function $g\\colon\\mathbb{R}\\rightarrow\\mathbb{R}$ that maps a value $x$, assumed by $X$, to a value $y=g\\left(x\\right)$, assumed by a RV $Y$, \n",
    "$$\n",
    "Y\\left(\\omega\\right)=g\\circ X\\left(\\omega\\right)=g\\left(X\\left(\\omega\\right)\\right)\n",
    "$$\n",
    "It is customary to speak of $Y$ as a transformation of $X$ under $g$ and write $Y=g\\left(X\\right)$. It is important to remember that $g$ takes values of $X$ and, hence should not be viewed as something that takes functions and returns functions, a much more complicated concept that will be encountered later in this course. In this particular example, the transformation takes the form\n",
    "$$\n",
    "Y=a+\\left(b-a\\right)X=a+\\left(b-a\\right)\\mathbf{1}_{\\mathsf{A}}\n",
    "$$\n",
    "The probability measure\n",
    "$$\n",
    "P_{X}=\\mathrm{Ber}_{p}=\\left(1-p\\right)\\delta_{0}+p\\delta_{1}\n",
    "$$\n",
    "transforms to\n",
    "$$\n",
    "P_{Y}=\\left(1-p\\right)\\delta_{a}+p\\delta_{b}\n",
    "$$\n",
    "This is an example of a two-point distribution. Since we assumed $b>a$, the correspondence of the events can be established.\n",
    "\\begin{align*}\n",
    "\\left(Y=k\\right) & =\\left(a+\\left(b-a\\right)X=k\\right)=\\left(X=\\frac{k-a}{b-a}\\right)\\\\\n",
    "\\left(Y\\le k\\right) & =\\left(a+\\left(b-a\\right)X\\le k\\right)=\\left(X\\le\\frac{k-a}{b-a}\\right)\n",
    "\\end{align*}\n",
    "That leads to the following characterization of $Y$:\n",
    "$$\n",
    "\\mathcal{Y}=\\left\\{ a,b\\right\\} ,\\ f_{Y}^{\\#}\\left(k\\right)=f_{X}^{\\#}\\left(\\frac{k-a}{b-a}\\right),\\ F_{Y}^{\\#}\\left(k\\right)=F_{X}^{\\#}\\left(\\frac{k-a}{b-a}\\right)\n",
    "$$\n",
    "The PDF and CDF can then be recovered.\n",
    "$$\n",
    "f_{Y}\\left(y\\right)=f_{Y}^{\\#}\\left(y\\right)\\left[y\\in\\left\\{ a,b\\right\\} \\right],\\ F_{Y}\\left(y\\right)=F_{X}^{\\#}\\left(\\left[y\\ge b\\right]\\right)\\left[y\\ge a\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "graffitiCellId": "id_jg2qy00"
   },
   "source": [
    "#### Typical value of a distribution\n",
    "Consider a discrete distribution of probabilities $p_{k}=P\\left(\\left\\{ k\\right\\} \\right)$\n",
    "and support $\\mathcal{X}^{\\#}$. If the distribution is symmetrical about a single peak, then the most likely value, called the mode, can be considered a typical value. For distributions of other shapes, the mode may not be appropriate. Other alternatives include the median and the mean. The median $m$ of a distribution generalizes the notion of middle value and is defined as the value at which the CDF equals $1/2$. The mean $\\mu$ of a distribution is the value that best fits long-run observations in a least squared sense. We can get an estimate $\\hat{\\mu}_{n}$ of the mean based on the sum of its squared Euclidean distances from observations in $n$ trials of the underlying experiment, treated as a function of $\\hat{\\mu}_{n}$,\n",
    "$$\n",
    "S\\left(\\hat{\\mu}_{n}\\right)=\\sum_{k\\in\\mathcal{X}^{\\#}}\\left(k-\\hat{\\mu}_{n}\\right)^{2}a_{n}\\left(k\\right)\n",
    "$$\n",
    "where $a_{n}\\left(k\\right)$ is the observed frequency of $k$ in $n$ trials. Note that absolute frequencies must add up to $n$. Since $\\hat{\\mu}_{n}$ minimizes $S\\left(\\hat{\\mu}_{n}\\right)$ by definition, we require $S^{\\prime}\\left(\\hat{\\mu}_{n}\\right)=0$ which leads to\n",
    "$$\n",
    "-2\\sum_{k\\in\\mathcal{X}^{\\#}}\\left(k-\\hat{\\mu}_{n}\\right)a_{n}\\left(k\\right)=0\\implies n\\hat{\\mu}_{n}=\\sum_{k\\in\\mathcal{X}^{\\#}}ka_{n}\\left(k\\right)\\implies\\hat{\\mu}_{n}=\\frac{1}{n}\\sum_{k\\in\\mathcal{X}^{\\#}}ka_{n}\\left(k\\right)=\\sum_{k\\in\\mathcal{X}^{\\#}}kr_{n}\\left(k\\right)\n",
    "$$\n",
    "where $r_{k}\\left(n\\right)$ is the relative frequency of $k$ in $n$ trials. Since the long-run relative frequencies $r_{n}\\left(k\\right)$ approach probabilities $p_{k}$, we have the following formula for the mean of a discrete distribution\n",
    "$$\n",
    "\\mu=\\sum_{k\\in\\mathcal{X}^{\\#}}kp_{k}\n",
    "$$\n",
    "This makes sense as each value $x_{i}$ of the RV $X$ is weighted by the associated probability $p_{i}$. Owing to this probability weighting, the mean is also called the expected value, or expectation, of the RV. The expectation of a RV $X$, denoted by $\\mathop{\\mathrm{E}}X$, is defined by\n",
    "$$\n",
    "\\mu_{X}=\\mathop{\\mathrm{E}}X=\\intop_{\\mathbb{R}}xP_{X}\\left(\\mathrm{d}\\mathsf{B}_{x}\\right)=\\sum_{k\\in\\mathcal{X}^{\\#}}kf^{\\#}\\left(k\\right)+\\intop_{\\mathcal{X}}xf^{\\wedge}\\left(x\\right)\\mathrm{d}x\n",
    "$$\n",
    "Note that the expectation is not a point function like a transformation. Instead, the expectation is an example of what is called an operator. An operator takes a function and returns another function. A special kind of an operator, called a functional, collapses a function to a number. You can now see that the expectation operator acts on a RV, a function, and returns the mean, a number. Thus, the expectation operator is a functional. The notation $\\mathop{\\mathrm{E}}X$ without using any brackets not only reminds of the operator involved but also helps to distinguish it from a transformation $g\\left(X\\right)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "graffitiCellId": "id_ou7x9gu"
   },
   "source": [
    "#### Expectation of a transformed RV\n",
    "Consider a transformation $Y=g\\left(X\\right)$ where $g$ is a point function making any value $x$ of $X$ to the value $g\\left(x\\right)$ of $Y$. How are the two expectations $\\mathop{\\mathrm{E}}X$ and $\\mathop{\\mathrm{E}}Y$ related? Let us work that out by using the above definition of expectation and noting that the differential regions $\\mathrm{d}\\mathsf{B}_{x}$ and $\\mathrm{d}\\mathsf{B}_{y}$ in the Borel field correspond to the same event in the original event space. Putting it together, we get\n",
    "$$\n",
    "\\mathop{\\mathrm{E}}Y=\\intop_{\\mathbb{R}}yP_{Y}\\left(\\mathrm{d}\\mathsf{B}_{y}\\right)=\\intop_{\\mathbb{R}}g\\left(x\\right)P_{X}\\left(\\mathrm{d}\\mathsf{B}_{x}\\right)=\\sum_{k\\in\\mathcal{X}^{\\#}}g\\left(k\\right)f^{\\#}\\left(k\\right)+\\intop_{\\mathcal{X}}g\\left(x\\right)f^{\\wedge}\\left(x\\right)\\mathrm{d}x\n",
    "$$\n",
    "In other words, we do not need the distribution of $Y$ to compute the expectation. This makes sense in terms of long-run relative frequencies. The observed long-run relative frequency of a value $x$ is also the\n",
    "frequency $g\\left(x\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "graffitiCellId": "id_6d97bnd"
   },
   "source": [
    "Let us work out the mean for the two RVs discussed above.\n",
    "\n",
    ">**Mean of the Dirac distribution**\n",
    ">The mean of the deterministic RV $a\\mathbf{1}_{\\mathsf{\\Omega}}\\sim\\delta_{a}$ is\n",
    "$$\n",
    "\\mathop{\\mathrm{E}}a\\mathbf{1}_{\\mathsf{\\Omega}}=\\sum_{k\\in\\left\\{ a\\right\\} }k\\delta_{a}\\left(\\left\\{ k\\right\\} \\right)=a\n",
    "$$\n",
    "In other words, the expected value of a (deterministic) constant is the constant itself. Though the expectation operator is defined only for RVs, we will write $\\mathop{\\mathrm{E}}a$ as a short form of $\\mathop{\\mathrm{E}}a\\mathbf{1}_{\\mathsf{\\Omega}}$. Such an abuse of notation is allowed as long as you know the subtlety involved. In\n",
    "particular, the short form $\\mathop{\\mathrm{E}}a$ should not be used\n",
    "as a justification for the following wrong statements:\n",
    "> - $a$ is a deterministic RV\n",
    ">    - This is wrong because $a$ is only the constant value of the deterministic RV $a\\mathbf{1}_{\\mathsf{\\Omega}}$.\n",
    "> - The $\\mathop{\\mathrm{E}}$ operator is defined for constants.\n",
    ">    - The is wrong because the expectation is defined only for RVs.\n",
    ">\n",
    ">You will see such pieces of misunderstanding in the wild and I hope you can sift through that.\n",
    "\n",
    "#### Linearity of the expectation\n",
    "Recall from math and circuit courses that integration and summation are linear operations. You can verify from the above definition that the expectation inherits linearity from integration and summation in the following sense:\n",
    "$$\n",
    "\\mathop{\\mathrm{E}}\\left(aX+b\\right)=a\\mathop{\\mathrm{E}}X+\\mathop{\\mathrm{E}}b=a\\mathop{\\mathrm{E}}X+b\n",
    "$$\n",
    "Setting $a=1$ and $b=-\\mu_{X}$ gives \n",
    "$$\n",
    "\\mathop{\\mathrm{E}}\\left(X-\\mu_{X}\\right)=\\mathop{\\mathrm{E}}X-\\mu_{X}=0\n",
    "$$\n",
    "as expected.\n",
    "\n",
    ">**Mean of the Bernoulli distribution**\n",
    ">The mean of the Bernoulli RV $X\\sim\\mathrm{Ber}_{p}$ is\n",
    "$$\n",
    "\\mathop{\\mathrm{E}}X=\\sum_{k=0}^{1}k\\mathrm{Ber}\\left(k;p\\right)=\\sum_{k=0}^{1}kp^{k}\\left(1-p\\right)^{1-k}=p\n",
    "$$\n",
    "The mean of the transformed (two-point) RV $Y=a+\\left(b-a\\right)X$\n",
    "follows from linearity\n",
    "$$\n",
    "\\mathop{\\mathrm{E}}Y=a+\\left(b-a\\right)\\mathrm{E}X=a+\\left(b-a\\right)p=\\left(1-p\\right)a+pb\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "graffitiCellId": "id_pb82llw"
   },
   "source": [
    "#### Spread of a distribution\n",
    "A measure of spread of a distribution is the so-called standard deviation, denoted $\\sigma$, which can be viewed as the Euclidean distance between a typical pair of values in a distribution. The mean was introduced as a typical value in the sense of minimizing the sum of its squared Euclidean distances from long-run observations. That gives us a hint of an estimate $\\hat{\\sigma}_{n}^{2}$ of the squared standard deviation--the average squared distance per trial, \n",
    "$$\n",
    "\\hat{\\sigma}_{n}^{2}=\\frac{1}{n}S\\left(\\hat{\\mu}_{n}\\right)=\\frac{1}{n}\\sum_{k\\in\\mathcal{X}^{\\#}}\\left(k-\\hat{\\mu}_{n}\\right)^{2}a_{n}\\left(k\\right)=\\sum_{k\\in\\mathcal{X}^{\\#}}\\left(k-\\hat{\\mu}_{n}\\right)^{2}r_{n}\\left(k\\right)\n",
    "$$\n",
    "A more elaborate approach, however, is to study the average squared Euclidean distances between all pairs of long-run observations,\n",
    "\\begin{align*}\n",
    "\\frac{1}{2n^{2}}\\sum_{j\\in\\mathcal{X}^{\\#}}\\sum_{i\\in\\mathcal{X}^{\\#}}\\left(i-j\\right)^{2}a_{n}\\left(i\\right)a_{n}\\left(j\\right) & =\\frac{1}{2}\\sum_{j\\in\\mathcal{X}^{\\#}}\\sum_{i\\in\\mathcal{X}^{\\#}}\\left(i-\\hat{\\mu}_{n}+j-\\hat{\\mu}_{n}\\right)^{2}r_{n}\\left(i\\right)r_{n}\\left(j\\right)\\\\\n",
    " & =\\frac{1}{2}\\sum_{j\\in\\mathcal{X}^{\\#}}\\left[\\sum_{i\\in\\mathcal{X}^{\\#}}\\left(i-\\hat{\\mu}_{n}\\right)^{2}r_{n}\\left(i\\right)\\right]r_{n}\\left(j\\right)+\\frac{1}{2}\\sum_{i\\in\\mathcal{X}^{\\#}}\\left[\\sum_{j\\in\\mathcal{X}^{\\#}}\\left(j-\\hat{\\mu}_{n}\\right)^{2}r_{n}\\left(j\\right)\\right]r_{n}\\left(i\\right)\\\\\n",
    " & =\\sum_{k\\in\\mathcal{X}^{\\#}}\\left(k-\\hat{\\mu}_{n}\\right)^{2}r_{n}\\left(k\\right)\\\\\n",
    " & =\\hat{\\sigma}_{n}^{2}\n",
    "\\end{align*}\n",
    "Not shown in the above derivation is the following vanishing product:\n",
    "$$\n",
    "-\\left[\\sum_{i\\in\\mathcal{X}^{\\#}}\\left(i-\\hat{\\mu}_{n}\\right)r_{n}\\left(i\\right)\\right]\\left[\\sum_{j\\in\\mathcal{X}^{\\#}}\\left(j-\\hat{\\mu}_{n}\\right)r_{n}\\left(j\\right)\\right]=0\n",
    "$$\n",
    "Either way, $\\hat{\\sigma}_{n}^{2}$ represents the average squared variation. That leads us to define the standard deviation $\\sigma$ of a discrete distribution as the positive square root of what is called the variance of the distribution,\n",
    "$$\n",
    "\\sigma^{2}=\\sum_{k\\in\\mathcal{X}^{\\#}}\\left(k-\\hat{\\mu}\\right)^{2}p_{k}\n",
    "$$\n",
    "The variance of a general RV $X$, denoted $\\mathop{\\mathop{\\mathrm{Var}}}X$,\n",
    "is defined by\n",
    "$$\n",
    "\\sigma_{X}^{2}=\\mathop{\\mathop{\\mathrm{Var}}}X=\\mathop{\\mathrm{E}}\\left(X-\\mu_{X}\\right)^{2}\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "graffiti": {
   "firstAuthorId": "dev",
   "id": "id_t6vmhmu",
   "language": "EN"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

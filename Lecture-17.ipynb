{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Some LaTeX definitions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "$$\\newcommand{\\diff}{\\mathop{}\\!\\textnormal{d}}\n",
       "\\newcommand{\\euler}{\\mathrm{e}}\n",
       "\\DeclareMathOperator{\\PP}{P}\n",
       "\\DeclareMathOperator{\\EE}{E}\n",
       "\\DeclareMathOperator{\\Var}{Var}\n",
       "\\DeclareMathOperator{\\Ber}{Ber}\n",
       "\\DeclareMathOperator{\\Ud}{Ud}\n",
       "\\DeclareMathOperator{\\U}{U}\n",
       "\\DeclareMathOperator{\\ind}{\\mathbf{1}}\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run nbinitex.ipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "graffitiCellId": "id_0m3re3b"
   },
   "source": [
    "# EE319 - Probability & Random Processes\n",
    "## *Dr.-Ing. Mukhtar Ullah*, FAST NUCES, Spring 2020\n",
    "<hr>\n",
    "\n",
    "## **Lecture 17** (2020-04-xx)\n",
    "## Random Variables (continued)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Bernoulli Measure (continued)\n",
    "#### Indicator function\n",
    "The Iversion bracket has been so far employed in two different contexts.\n",
    "Let us compare these two definitions\n",
    "\\\\[\n",
    "X\\left(\\omega\\right)=\\left[\\omega\\in\\mathsf{A}\\right],\\quad\\text{vs}\\quad\\delta_{a}\\left(\\mathsf{B}\\right)=\\left[a\\in\\mathsf{B}\\right]\n",
    "\\\\]\n",
    "On the left you see Bernoulli RV that indicates the occurrence of $\\mathsf{A}$. On the right you see the Dirac measure concentrating all the probability at $a$. It is important to see the difference in meaning despite\n",
    "the usage of the same Iversion bracket. One (on the left) is a point function and the other (on the right) is a set function. The symbol $\\delta_{a}$ identifies uniquely the Dirac measure and we promise not to use it for any other purpose. On the other hand, we cannot restrict the usage of $X$ to a particular point function. We need a dedicated notation for viewing $\\left[\\omega\\in\\mathsf{A}\\right]$ as a point function. This motivates the following definition. The indicator function of a set $\\mathsf{A}$ is a point function defined by\n",
    "\\\\[\n",
    "\\ind_{\\mathsf{A}}\\left(\\omega\\right)=\\left[\\omega\\in\\mathsf{A}\\right]\n",
    "\\\\]\n",
    "Thus, every Bernoulli RV is an indicator function. \n",
    "\n",
    ">Had we known this before, a compact definition of Bernoulli RV $X\\sim\\Ber_{p}$\n",
    "would read $X=\\ind_{\\mathsf{A}}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "graffitiCellId": "id_73grsjk"
   },
   "source": [
    "#### From the Bernoulli family to the two-point family\n",
    "The Bernoulli RV $X\\sim\\Ber_{p}$ assigns the value $1$ to (the success) $\\mathsf{A}$ and $0$ to (the failure) $\\mathsf{A^{c}}$. In a more general setting, the set $\\left\\{ 0,1\\right\\} $ could be replaced with $\\left\\{ a,b\\right\\} $. For convenience we will assume $b>a$. The resulting RV $Y$ can be defined with respect to the same event space $\\mathcal{F}$ by\n",
    "$$\n",
    "Y\\left(\\omega\\right)=a\\ind_{\\mathsf{A^{c}}}\\left(\\omega\\right)+b\\ind_{\\mathsf{A}}\\left(\\omega\\right)\n",
    "$$\n",
    "or, equivalently,\n",
    "$$\n",
    "Y\\left[\\mathsf{A^{c}}\\right]=\\left\\{ a\\right\\} ,\\ X\\left[\\mathsf{A}\\right]=\\left\\{ b\\right\\} \n",
    "$$\n",
    "It must be possible to derive $Y$ from $X$. How? You need not look further than the above definition. Note that the indicator function must obey\n",
    "$$\n",
    "\\ind_{\\mathsf{A}}\\left(\\omega\\right)+\\ind_{\\mathsf{A^{c}}}\\left(\\omega\\right)=1,\\quad\\omega\\in\\mathsf{A}\\cup\\mathsf{A^{c}}\n",
    "$$\n",
    "since $\\omega$ cannot belong to both $\\mathsf{A}$ and $\\mathsf{A^{c}}$. That simplifies the definition of $Y$ to\n",
    "$$\n",
    "Y\\left(\\omega\\right)=a+\\left(b-a\\right)\\ind_{\\mathsf{A}}\\left(\\omega\\right)=a+\\left(b-a\\right)X\\left(\\omega\\right)\n",
    "$$\n",
    "Consequently a pair $\\left(x,y\\right)$ of values assigned by the pair $\\left(X,Y\\right)$ of RVs to any outcome $\\omega$ cannot be arbitrary and must a point in the line represented by the equation\n",
    "$$\n",
    "y=a+\\left(b-a\\right)x\n",
    "$$\n",
    "This leads to a new concept: a transformation of a RV. A transformation of a RV $X$ is a point function $g\\colon\\mathbb{R}\\rightarrow\\mathbb{R}$ that maps a value $x$, assumed by $X$, to a value $y=g\\left(x\\right)$, assumed by a RV $Y$, \n",
    "$$\n",
    "Y\\left(\\omega\\right)=g\\circ X\\left(\\omega\\right)=g\\left(X\\left(\\omega\\right)\\right)\n",
    "$$\n",
    "It is customary to speak of $Y$ as a transformation of $X$ under $g$ and write $Y=g\\left(X\\right)$. It is important to remember that $g$ takes values of $X$ and, hence should not be viewed as something that takes functions and returns functions, a much more complicated concept that will be encountered later in this course. In this particular example, the transformation takes the form\n",
    "$$\n",
    "Y=a+\\left(b-a\\right)X=a+\\left(b-a\\right)\\ind_{\\mathsf{A}}\n",
    "$$\n",
    "The probability measure\n",
    "$$\n",
    "P_{X}=\\Ber_{p}=\\left(1-p\\right)\\delta_{0}+p\\delta_{1}\n",
    "$$\n",
    "transforms to\n",
    "$$\n",
    "P_{Y}=\\left(1-p\\right)\\delta_{a}+p\\delta_{b}\n",
    "$$\n",
    "This is an example of a two-point distribution. Since we assumed $b>a$, the correspondence of the events can be established.\n",
    "\\begin{align*}\n",
    "\\left(Y=k\\right) & =\\left(a+\\left(b-a\\right)X=k\\right)=\\left(X=\\frac{k-a}{b-a}\\right)\\\\\n",
    "\\left(Y\\le k\\right) & =\\left(a+\\left(b-a\\right)X\\le k\\right)=\\left(X\\le\\frac{k-a}{b-a}\\right)\n",
    "\\end{align*}\n",
    "That leads to the following characterization of $Y$.\n",
    ">Support:\n",
    ">\\\\[\n",
    "\\mathcal{Y}=\\left\\{ a,b\\right\\} \n",
    "\\\\]\n",
    "Strictly positive density:\n",
    "\\\\[\n",
    "f_{Y}^{\\#}\\left(k\\right)=f_{X}^{\\#}\\left(\\frac{k-a}{b-a}\\right)=\\left[p^{k}\\left(1-p\\right)^{1-k}\\right]_{k=\\frac{k-a}{b-a}}=p^{\\frac{k-a}{b-a}}\\left(1-p\\right)^{\\frac{b-k}{b-a}}\n",
    "\\\\]\n",
    "Strictly increasing distribution:\n",
    "\\\\[\n",
    "F_{Y}^{\\#}\\left(k\\right)=F_{X}^{\\#}\\left(\\frac{k-a}{b-a}\\right)=\\left[\\left(1-p\\right)^{1-k}\\right]_{k=\\frac{k-a}{b-a}}=\\left(1-p\\right)^{\\frac{b-k}{b-a}}\n",
    "\\\\]\n",
    "\n",
    "The PDF and CDF can then be recovered from the strictly defined point functions above.\n",
    "\\begin{align*}\n",
    "f_{Y}\\left(y\\right) & =f_{Y}^{\\#}\\left(y\\right)\\left[y\\in\\left\\{ a,b\\right\\} \\right]=p^{\\frac{y-a}{b-a}}\\left(1-p\\right)^{\\frac{b-y}{b-a}}\\left[y\\in\\left\\{ a,b\\right\\} \\right]\\\\\n",
    "F_{Y}\\left(y\\right) & =F_{Y}^{\\#}\\left(\\max\\mathcal{Y}\\cap\\left(-\\infty,y\\right]\\right)\\left[\\mathcal{Y}\\cap\\left(-\\infty,y\\right]\\ne\\emptyset\\right]=\\left(1-p\\right)^{\\left[y<b\\right]}\\left[y\\ge a\\right]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### From two-point to several-point distribution\n",
    "Consider a probability space $\\left(\\mathsf{\\Omega},\\mathcal{F},P\\right)$ with the event space $\\mathcal{F}=\\sigma\\left(\\mathcal{C}\\right)$ generated by a partition $\\mathcal{C}=\\left\\{ \\mathsf{A}_{k}\\mid k\\in\\left[1\\cdot\\cdot n\\right]\\right\\} $ of the outcome space $\\mathsf{\\Omega}$, and a discrete probability measure $P$ defined by $P\\left(\\mathsf{A}_{k}\\right)=p_{k}$ for all $k$. Any RV $X$ definable on $\\mathsf{\\Omega}$ with respect to $\\mathcal{F}$ must assign $n$ different values so that the same value, say $k$, is assigned to all points in $\\mathsf{A}_{k}$. This RV $X$ turns out to be a weighted sum of $n$ indicator functions,\n",
    "namely \n",
    "\\\\[\n",
    "X\\left(\\omega\\right)=\\sum_{k=1}^{n}k\\ind_{\\mathsf{A}_{k}}\\left(\\omega\\right),\n",
    "\\\\]\n",
    "resulting in the image sets\n",
    "\\\\[\n",
    "X\\left[\\mathsf{A}_{k}\\right]=\\left\\{ k\\right\\} ,\\quad k\\in\\left[1\\cdot\\cdot n\\right]\n",
    "\\\\]\n",
    "The indicator functions are all Bernoulli distributed, $\\ind_{\\mathsf{A}_{k}}\\sim\\Ber_{p_{k}}$, and must satisfy\n",
    "\\\\[\n",
    "\\sum_{k=1}^{n}\\ind_{\\mathsf{A}_{k}}\\left(\\omega\\right)=1,\\quad\\omega\\in\\mathsf{\\Omega}\n",
    "\\\\]\n",
    " Any Borel set $\\mathsf{B}$ can be expressed as a disjoint union\n",
    "\\\\[\n",
    "\\mathsf{B}=\\bigcup_{k=1}^{n}\\left(\\mathsf{B}\\cap\\left\\{ k\\right\\} \\right)\\cup\\left(\\mathsf{B}\\setminus\\left[1\\cdot\\cdot n\\right]\\right)\n",
    "\\\\]\n",
    "with pre-image\n",
    "\\\\[\n",
    "X^{-1}\\left[\\mathsf{B}\\right]=\\bigcup_{k=1}^{n}\\left(\\mathsf{A}_{k}\\cap\\left\\{ k\\in\\mathsf{B}\\right\\} \\right)=\\bigcup_{k\\in\\mathsf{B}}\\mathsf{A}_{k}\n",
    "\\\\]\n",
    "and probability\n",
    "\\\\[\n",
    "P_{X}\\left(\\mathsf{B}\\right)=P\\left(X^{-1}\\left[\\mathsf{B}\\right]\\right)=\\sum_{k=1}^{n}P\\left(\\mathsf{A}_{k}\\right)\\left[k\\in\\mathsf{B}\\right]=\\sum_{k=1}^{n}p_{k}\\delta_{k}\\left(\\mathsf{B}\\right)\n",
    "\\\\]\n",
    "This gives us an $n$-point discrete measure, a weighted sum of $n$ Dirac measures, namely,\n",
    "\\\\[\n",
    "P_{X}=\\sum_{k=1}^{n}p_{k}\\delta_{k}\n",
    "\\\\]\n",
    "The associated probability distribution is an $n$-point discrete distribution with the following characterization:\n",
    "\\\\[\n",
    "\\mathcal{X}=\\left[1\\cdot\\cdot n\\right],\\ f_{X}^{\\#}\\left(k\\right)=p_{k},\\ F_{X}^{\\#}\\left(k\\right)=\\sum_{i=1}^{k}p_{i}\n",
    "\\\\]\n",
    "The PDF and CDF can then be recovered from the strictly defined point functions.\n",
    "\\begin{align*}\n",
    "f_{X}\\left(x\\right) & =f_{X}^{\\#}\\left(x\\right)\\left[x\\in\\left[1\\cdot\\cdot n\\right]\\right]\\\\\n",
    "F_{X}\\left(x\\right) & =F_{X}^{\\#}\\left(\\min\\left\\{ n,\\left\\lfloor x\\right\\rfloor \\right\\} \\right)\\left[x\\ge1\\right]\n",
    "\\end{align*}\n",
    "\n",
    ">**Discrete uniform distribution**\n",
    "A special case of the $n$-point distribution arises if $p_{k}=1/n$ for all $k$, that is, the probability is uniformly distributed among the $n$ points. A discrete uniform measure over the (first) $n$ natural numbers is defined by\n",
    "\\\\[\n",
    "\\Ud_{n}=\\frac{1}{n}\\sum_{k=1}^{n}\\delta_{k}\n",
    "\\\\]\n",
    "The associated probability distribution is called the discrete uniform distribution. That $X$ follows the discrete uniform distribution in the integer interval $\\left[1\\cdot\\cdot n\\right]$ is written as $X\\sim\\Ud_{n}$ with the following characterization:\n",
    "\\\\[\n",
    "\\mathcal{X}=\\left[1\\cdot\\cdot n\\right],\\ f_{X}^{\\#}\\left(k\\right)=\\mathrm{Ud}\\left(k\\mid n\\right)=\\frac{1}{n},\\ F_{X}^{\\#}\\left(k\\right)=\\frac{k}{n}\n",
    "\\\\]\n",
    "The PDF and CDF can then be recovered.\n",
    "\\\\[\n",
    "f_{X}\\left(x\\right)=f_{X}^{\\#}\\left(x\\right)\\left[x\\in\\left[1\\cdot\\cdot n\\right]\\right],\\ F_{X}\\left(x\\right)=F_{X}^{\\#}\\left(\\min\\left\\{ n,\\left\\lfloor x\\right\\rfloor \\right\\} \\right)\\left[x\\ge1\\right]\n",
    "\\\\]\n",
    ">\n",
    "<img src=\"images/uniform_pdf.png\" width=\"50%\" />\n",
    "<img src=\"images/uniform_cdf.png\" width=\"50%\" />\n",
    "\n",
    ">**Transformation of a discrete uniform RV**\n",
    "The discrete uniform RV $X\\sim\\Ud_{n}$, defined with respect\n",
    "to the event space $\\mathcal{F}$, takes values in $\\left[1\\cdot\\cdot n\\right]$.\n",
    "In a general setting, we could imagine a discrete uniform RV $Y\\sim\\Ud_{a,b}$,\n",
    "defined with respect to the same $\\mathcal{F}$ and taking values\n",
    "in $\\left[a\\cdot\\cdot b\\right]$ with $b=a-1+n$ so that the number\n",
    "of points remains the same. We can view the new RV $Y$ as the transformed\n",
    "RV \n",
    "\\\\[\n",
    "Y=a-1+X\n",
    "\\\\]\n",
    "The probability measure\n",
    "\\\\[\n",
    "P_{X}=\\Ud_{n}=\\frac{1}{n}\\sum_{k=1}^{n}\\delta_{k}\n",
    "\\\\]\n",
    "transforms to\n",
    "\\\\[\n",
    "P_{Y}=\\frac{1}{n}\\sum_{k=a}^{b}\\delta_{k}\n",
    "\\\\]\n",
    "This gives the generic discrete distribution. The correspondence of\n",
    "the events \n",
    "\\begin{align*}\n",
    "\\left(Y=k\\right) & =\\left(a-1+X=k\\right)=\\left(X=k-a+1\\right)\\\\\n",
    "\\left(Y\\le k\\right) & =\\left(a-1+X\\le k\\right)=\\left(X\\le k-a+1\\right)\n",
    "\\end{align*}\n",
    "leads to the following characterization of $Y$,\n",
    "\\\\[\n",
    "\\mathcal{Y}=\\left[a\\cdot\\cdot b\\right],\\ f_{Y}^{\\#}\\left(k\\right)=f_{X}^{\\#}\\left(k-a+1\\right)=\\frac{1}{b-a+1},\\ F_{Y}^{\\#}\\left(k\\right)=F_{X}^{\\#}\\left(k-a+1\\right)=\\frac{k-a+1}{b-a+1}\n",
    "\\\\]\n",
    "The PDF and CDF can then be recovered from the strictly defined functions.\n",
    "\\\\[\n",
    "f_{Y}\\left(y\\right)=f_{Y}^{\\#}\\left(y\\right)\\left[y\\in\\left[a\\cdot\\cdot b\\right]\\right],\\ F_{Y}\\left(y\\right)=F_{Y}^{\\#}\\left(\\min\\left\\{ b,\\left\\lfloor y\\right\\rfloor \\right\\} \\right)\\left[y\\ge a\\right]\n",
    "\\\\]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "graffitiCellId": "id_jg2qy00"
   },
   "source": [
    "#### Typical value of a distribution\n",
    "Consider a discrete distribution with values $k\\in\\mathcal{X}^{\\#}$ and probabilities $p_{k}$. If the distribution is symmetrical about a single peak, then the most likely value, called the mode, can be considered a typical value. For distributions of other shapes, the mode may not be appropriate. Other alternatives include the median and the mean. The median $m$ of a distribution generalizes the notion of middle value and is defined as the value at which the CDF equals $1/2$. The mean $\\mu$ of a distribution is the value that best fits long-run observations in a least squared sense. We can get an estimate $\\hat{\\mu}_{n}$ of the mean based on the sum of its squared Euclidean distances from observations in $n$ trials of the underlying experiment, treated as a function of $\\hat{\\mu}_{n}$,\n",
    "$$\n",
    "S\\left(\\hat{\\mu}_{n}\\right)=\\sum_{k\\in\\mathcal{X}^{\\#}}\\left(k-\\hat{\\mu}_{n}\\right)^{2}a_{n}\\left(k\\right)\n",
    "$$\n",
    "where $a_{n}\\left(k\\right)$ is the observed frequency of $k$ in $n$ trials. Note that absolute frequencies must add up to $n$. Since $\\hat{\\mu}_{n}$ minimizes $S\\left(\\hat{\\mu}_{n}\\right)$ by definition, we require $S^{\\prime}\\left(\\hat{\\mu}_{n}\\right)=0$ which leads to\n",
    "$$\n",
    "-2\\sum_{k\\in\\mathcal{X}^{\\#}}\\left(k-\\hat{\\mu}_{n}\\right)a_{n}\\left(k\\right)=0\\implies n\\hat{\\mu}_{n}=\\sum_{k\\in\\mathcal{X}^{\\#}}ka_{n}\\left(k\\right)\\implies\\hat{\\mu}_{n}=\\frac{1}{n}\\sum_{k\\in\\mathcal{X}^{\\#}}ka_{n}\\left(k\\right)=\\sum_{k\\in\\mathcal{X}^{\\#}}kr_{n}\\left(k\\right)\n",
    "$$\n",
    "where $r_{k}\\left(n\\right)$ is the relative frequency of $k$ in $n$ trials. Since the long-run relative frequencies $r_{n}\\left(k\\right)$ approach probabilities $p_{k}$, we have the following formula for the mean of a discrete distribution\n",
    "$$\n",
    "\\mu=\\sum_{k\\in\\mathcal{X}^{\\#}}kp_{k}\n",
    "$$\n",
    "This makes sense as each value $x_{i}$ of the RV $X$ is weighted by the associated probability $p_{i}$. Owing to this probability weighting, the mean is also called the expected value, or expectation, of the RV. The expectation of a RV $X$, denoted by $\\mathop{\\mathrm{E}}X$, is defined by\n",
    "$$\n",
    "\\mu_{X}=\\mathop{\\mathrm{E}}X=\\intop_{\\mathbb{R}}xP_{X}\\left(\\mathrm{d}\\mathsf{B}_{x}\\right)=\\sum_{k\\in\\mathcal{X}^{\\#}}kf^{\\#}\\left(k\\right)+\\intop_{\\mathcal{X}}xf^{\\wedge}\\left(x\\right)\\mathrm{d}x\n",
    "$$\n",
    "Note that the expectation is not a point function like a transformation. Instead, the expectation is an example of what is called an operator. An operator takes a function and returns another function. A special kind of an operator, called a functional, collapses a function to a number. You can now see that the expectation operator acts on a RV, a function, and returns the mean, a number. Thus, the expectation operator is a functional. The notation $\\mathop{\\mathrm{E}}X$ without using any brackets not only reminds of the operator involved but also helps to distinguish it from a transformation $g\\left(X\\right)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "graffitiCellId": "id_ou7x9gu"
   },
   "source": [
    "#### Expectation of a transformed RV\n",
    "Consider a transformation $Y=g\\left(X\\right)$ where $g$ is a point function making any value $x$ of $X$ to the value $g\\left(x\\right)$ of $Y$. How are the two expectations $\\mathop{\\mathrm{E}}X$ and $\\mathop{\\mathrm{E}}Y$ related? Let us work that out by using the above definition of expectation and noting that the differential regions $\\mathrm{d}\\mathsf{B}_{x}$ and $\\mathrm{d}\\mathsf{B}_{y}$ in the Borel field correspond to the same event in the original event space. Putting it together, we get\n",
    "$$\n",
    "\\mathop{\\mathrm{E}}Y=\\intop_{\\mathbb{R}}yP_{Y}\\left(\\mathrm{d}\\mathsf{B}_{y}\\right)=\\intop_{\\mathbb{R}}g\\left(x\\right)P_{X}\\left(\\mathrm{d}\\mathsf{B}_{x}\\right)=\\sum_{k\\in\\mathcal{X}^{\\#}}g\\left(k\\right)f^{\\#}\\left(k\\right)+\\intop_{\\mathcal{X}}g\\left(x\\right)f^{\\wedge}\\left(x\\right)\\mathrm{d}x\n",
    "$$\n",
    "In other words, we do not need the distribution of $Y$ to compute the expectation. This makes sense in terms of long-run relative frequencies. The observed long-run relative frequency of a value $x$ is also the\n",
    "frequency $g\\left(x\\right)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "graffitiCellId": "id_6d97bnd"
   },
   "source": [
    "Let us work out the mean for the two RVs discussed above.\n",
    "\n",
    ">**Mean of the Dirac distribution**\n",
    ">The mean of the deterministic RV $a\\sim\\delta_{a}$ is\n",
    "$$\n",
    "\\mathop{\\mathrm{E}}a=\\sum_{k\\in\\left\\{ a\\right\\} }k\\delta_{a}\\left(\\left\\{ k\\right\\} \\right)=a\n",
    "$$\n",
    "In other words, the expected value of a (deterministic) constant is the constant itself.\n",
    "\n",
    "#### Linearity of the expectation\n",
    "Recall from math and circuit courses that integration and summation are linear operations. You can verify from the above definition that the expectation inherits linearity from integration and summation in the following sense:\n",
    "$$\n",
    "\\mathop{\\mathrm{E}}\\left(aX+b\\right)=a\\mathop{\\mathrm{E}}X+\\mathop{\\mathrm{E}}b=a\\mathop{\\mathrm{E}}X+b\n",
    "$$\n",
    "Setting $a=1$ and $b=-\\mu_{X}$ gives \n",
    "$$\n",
    "\\mathop{\\mathrm{E}}\\left(X-\\mu_{X}\\right)=\\mathop{\\mathrm{E}}X-\\mu_{X}=0\n",
    "$$\n",
    "as expected.\n",
    "\n",
    ">**Mean of the Bernoulli distribution**\n",
    ">The mean of the Bernoulli RV $X\\sim\\Ber_{p}$ is\n",
    "$$\n",
    "\\mathop{\\mathrm{E}}X=\\sum_{k=0}^{1}k\\mathrm{Ber}\\left(k;p\\right)=\\sum_{k=0}^{1}kp^{k}\\left(1-p\\right)^{1-k}=p\n",
    "$$\n",
    "The mean of the transformed (two-point) RV $Y=a+\\left(b-a\\right)X$\n",
    "follows from linearity\n",
    "$$\n",
    "\\mathop{\\mathrm{E}}Y=a+\\left(b-a\\right)\\mathrm{E}X=a+\\left(b-a\\right)p=\\left(1-p\\right)a+pb\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "graffitiCellId": "id_pb82llw"
   },
   "source": [
    "#### Spread of a distribution\n",
    "A measure of spread of a distribution is the so-called standard deviation, denoted $\\sigma$, which can be viewed as the Euclidean distance between a typical pair of values in a distribution. The mean was introduced as a typical value in the sense of minimizing the sum of its squared Euclidean distances from long-run observations. That gives us a hint of an estimate $\\hat{\\sigma}_{n}^{2}$ of the squared standard deviation--the average squared distance per trial, \n",
    "$$\n",
    "\\hat{\\sigma}_{n}^{2}=\\frac{1}{n}S\\left(\\hat{\\mu}_{n}\\right)=\\frac{1}{n}\\sum_{k\\in\\mathcal{X}^{\\#}}\\left(k-\\hat{\\mu}_{n}\\right)^{2}a_{n}\\left(k\\right)=\\sum_{k\\in\\mathcal{X}^{\\#}}\\left(k-\\hat{\\mu}_{n}\\right)^{2}r_{n}\\left(k\\right)\n",
    "$$\n",
    "A more elaborate approach, however, is to study the average squared Euclidean distances between all pairs of long-run observations,\n",
    "\\begin{align*}\n",
    "\\frac{1}{2n^{2}}\\sum_{j\\in\\mathcal{X}^{\\#}}\\sum_{i\\in\\mathcal{X}^{\\#}}\\left(i-j\\right)^{2}a_{n}\\left(i\\right)a_{n}\\left(j\\right) & =\\frac{1}{2}\\sum_{j\\in\\mathcal{X}^{\\#}}\\sum_{i\\in\\mathcal{X}^{\\#}}\\left(i-\\hat{\\mu}_{n}+j-\\hat{\\mu}_{n}\\right)^{2}r_{n}\\left(i\\right)r_{n}\\left(j\\right)\\\\\n",
    " & =\\frac{1}{2}\\sum_{j\\in\\mathcal{X}^{\\#}}\\left[\\sum_{i\\in\\mathcal{X}^{\\#}}\\left(i-\\hat{\\mu}_{n}\\right)^{2}r_{n}\\left(i\\right)\\right]r_{n}\\left(j\\right)+\\frac{1}{2}\\sum_{i\\in\\mathcal{X}^{\\#}}\\left[\\sum_{j\\in\\mathcal{X}^{\\#}}\\left(j-\\hat{\\mu}_{n}\\right)^{2}r_{n}\\left(j\\right)\\right]r_{n}\\left(i\\right)\\\\\n",
    " & =\\sum_{k\\in\\mathcal{X}^{\\#}}\\left(k-\\hat{\\mu}_{n}\\right)^{2}r_{n}\\left(k\\right)\\\\\n",
    " & =\\hat{\\sigma}_{n}^{2}\n",
    "\\end{align*}\n",
    "Not shown in the above derivation is the following vanishing product:\n",
    "$$\n",
    "-\\left[\\sum_{i\\in\\mathcal{X}^{\\#}}\\left(i-\\hat{\\mu}_{n}\\right)r_{n}\\left(i\\right)\\right]\\left[\\sum_{j\\in\\mathcal{X}^{\\#}}\\left(j-\\hat{\\mu}_{n}\\right)r_{n}\\left(j\\right)\\right]=0\n",
    "$$\n",
    "Either way, $\\hat{\\sigma}_{n}^{2}$ represents the average squared variation. That leads us to define the standard deviation $\\sigma$ of a discrete distribution as the positive square root of what is called the variance of the distribution,\n",
    "$$\n",
    "\\sigma^{2}=\\sum_{k\\in\\mathcal{X}^{\\#}}\\left(k-\\hat{\\mu}\\right)^{2}p_{k}\n",
    "$$\n",
    "The variance of a general RV $X$, denoted $\\mathop{\\mathop{\\mathrm{Var}}}X$,\n",
    "is defined by\n",
    "$$\n",
    "\\sigma_{X}^{2}=\\mathop{\\mathop{\\mathrm{Var}}}X=\\mathop{\\mathrm{E}}\\left(X-\\mu_{X}\\right)^{2}\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

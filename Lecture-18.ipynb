{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Some LaTeX definitions\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "$$\\newcommand{\\diff}{\\mathop{}\\!\\mathrm{d}}\n",
       "\\DeclareMathOperator{\\Diff}{D}\n",
       "\\newcommand{\\euler}{\\mathrm{e}}\n",
       "\\DeclareMathOperator{\\EE}{E}\n",
       "\\DeclareMathOperator{\\Var}{Var}\n",
       "\\DeclareMathOperator{\\Cov}{Cov}\n",
       "\\DeclareMathOperator{\\Ber}{Ber}\n",
       "\\DeclareMathOperator{\\Bin}{Bin}\n",
       "\\DeclareMathOperator{\\NB}{NB}\n",
       "\\DeclareMathOperator{\\Geo}{Geo}\n",
       "\\DeclareMathOperator{\\HG}{HG}\n",
       "\\DeclareMathOperator{\\Poi}{Poi}\n",
       "\\DeclareMathOperator{\\Ud}{Ud}\n",
       "\\DeclareMathOperator{\\U}{U}\n",
       "\\DeclareMathOperator{\\ind}{\\mathbf{1}}\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#HIDDEN\n",
    "%run nbinitex.ipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "graffitiCellId": "id_0m3re3b"
   },
   "source": [
    "# EE319 - Probability & Random Processes\n",
    "## *Dr.-Ing. Mukhtar Ullah*, FAST NUCES, Spring 2020\n",
    "<hr>\n",
    "\n",
    "## **Lecture 18** (2020-04-xx)\n",
    "## Several-point distribution (continued)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### Throw of two dice\n",
    "Recall the two-dice throw experiment with sample space\n",
    "\\\\[\n",
    "\\mathsf{\\Omega}=\\left[1\\cdot\\cdot6\\right]^{2}=\\left[1\\cdot\\cdot6\\right]\\times\\left[1\\cdot\\cdot6\\right]\n",
    "\\\\]\n",
    "and two partitions as illustrated below.\n",
    "<center><img src=\"images/two_dice_partition_wrt_sum.png\" width=\"70%\"></center>\n",
    "    \n",
    "The partition on the left is a detailed one. Say our interest lies in the total number of dots facing up on both dice.\n",
    "    \n",
    "Events of interest\n",
    "\\\\[\n",
    "\\mathsf{A}_{k}=\\left\\{ \\left(i,k-i\\right)\\mid i\\in\\left[1\\cdot\\cdot6\\right]\\right\\} ,\\quad k\\in\\left[2\\cdot\\cdot12\\right]\n",
    "\\\\]\n",
    "Probabilities\n",
    "\\\\[\n",
    "p_{k}=P\\left(\\mathsf{A}_{k}\\right)=\\frac{\\left|\\mathsf{A}_{k}\\right|}{\\left|\\mathsf{\\Omega}\\right|}=\\begin{cases}\n",
    "\\dfrac{k-1}{36} & k\\in\\left[2\\cdot\\cdot7\\right]\\\\\n",
    "\\dfrac{13-k}{36} & k\\in\\left[8\\cdot\\cdot12\\right]\n",
    "\\end{cases}\n",
    "\\\\]\n",
    "Define the appropriate RV\n",
    "\\\\[\n",
    "X=\\sum_{k=2}^{12}k\\ind_{\\mathsf{A}_{k}},\\quad X\\left[\\mathsf{A}_{k}\\right]=\\left\\{ k\\right\\} ,\\quad k\\in\\left[2\\cdot\\cdot12\\right]\n",
    "\\\\]\n",
    "Induced probability measure\n",
    "\\\\[\n",
    "P_{X}=\\sum_{k=1}^{n}p_{k}\\delta_{k}\n",
    "\\\\]\n",
    "Induced probability distribution\n",
    "\\\\[\n",
    "\\mathcal{X}=\\left[2\\cdot\\cdot12\\right],\\quad f_{X}^{\\#}\\left(k\\right)=p_{k},\\quad F_{X}^{\\#}\\left(k\\right)=\\sum_{i=2}^{k}p_{i}\n",
    "\\\\]\n",
    "PDF and CDF\n",
    "\\begin{align*}\n",
    "f_{X}\\left(x\\right) & =f_{X}^{\\#}\\left(x\\right)\\left[x\\in\\left[2\\cdot\\cdot12\\right]\\right]\\\\\n",
    "F_{X}\\left(x\\right) & =F_{X}^{\\#}\\left(\\min\\left\\{ 12,\\left\\lfloor x\\right\\rfloor \\right\\} \\right)\\left[x\\ge2\\right]\n",
    "\\end{align*}\n",
    "\n",
    "This was the distribution included in the very first lecture.\n",
    "    \n",
    "<center><img src=\"images/chance_regularity_histogram.png\" width=\"50%\"></center>\n",
    "\n",
    "Explain in your own words the pattern in this histogram in light of the PDF above.\n",
    "\n",
    "This example motivates a general formulation of a discrete distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Generic discrete distribution\n",
    "Consider a probability space $\\left(\\mathsf{\\Omega},\\mathcal{F},P\\right)$\n",
    "with the event space $\\mathcal{F}=\\sigma\\left(\\mathcal{C}\\right)$\n",
    "generated by a partition $\\mathcal{C}=\\left\\{ \\mathsf{A}_{k}\\mid k\\in\\mathcal{K}\\right\\} $\n",
    "of the outcome space $\\mathsf{\\Omega}$, and a discrete probability\n",
    "measure $P$ defined by $P\\left(\\mathsf{A}_{k}\\right)=p_{k}$ for\n",
    "all indices $k\\in\\mathcal{K}$. Any RV $X$ definable on $\\mathsf{\\Omega}$\n",
    "with respect to $\\mathcal{F}$ must assign same value, say $k$, to\n",
    "all points in $\\mathsf{A}_{k}$ but different from other values $j$.\n",
    "Mathematically, the definition of $X$ reads\n",
    "\\\\[\n",
    "X=\\sum_{k\\in\\mathcal{K}}k\\mathbf{1}_{\\mathsf{A}_{k}},\\quad X\\left[\\mathsf{A}_{k}\\right]=\\left\\{ k\\right\\} ,\\quad k\\in\\mathcal{K}\n",
    "\\\\]\n",
    "Following a procedure similar to that for other cases above, the generic\n",
    "discrete measure can be shown to take the form\n",
    "\\\\[\n",
    "P_{X}=\\sum_{k\\in\\mathcal{K}}p_{k}\\delta_{k}\n",
    "\\\\]\n",
    "That $X$ follows a discrete distribution is written as $X\\sim\\sum_{k\\in\\mathcal{K}}p_{k}\\delta_{k}$\n",
    "with the following characterization:\n",
    "\\\\[\n",
    "\\mathcal{X}=\\mathcal{K},\\ f_{X}^{\\#}\\left(k\\right)=p_{k},\\ F_{X}^{\\#}\\left(k\\right)=\\sum_{i\\in\\mathcal{X}}p_{k}\\left[i\\le k\\right]\n",
    "\\\\]\n",
    "The PDF and CDF can then be recovered.\n",
    "\\\\[\n",
    "f_{X}\\left(x\\right)=f_{X}^{\\#}\\left(x\\right)\\left[x\\in\\mathcal{X}\\right],\\ F_{X}\\left(x\\right)=F_{X}^{\\#}\\left(\\max\\mathcal{X}_{\\le x}\\right)\\left[\\mathcal{X}_{\\le x}\\ne\\emptyset\\right]\n",
    "\\\\]\n",
    "\n",
    "An illustration of the generic discrete RV will help.\n",
    "<center><img src=\"images/discrete_rv_mapping.png\" width=\"80%\"></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Bit transmission\n",
    "Recall the bit transmission problem illustrated here. \n",
    "<center><img src=\"images/bit_transmission.png\" width=\"50%\"></center>\n",
    "\n",
    "The input bit is modeled by a Bernoulli RV $X\\sim\\Ber_{1-p}$. The output bit is also a Bernoulli RV but with a parameter that we need to find. The diagram encodes the following conditional probabilities.\n",
    "\\begin{align*}\n",
    "P\\left(Y=1\\mid X=0\\right) & =P\\left(Y=0\\mid X=1\\right)=\\epsilon\\\\\n",
    "P\\left(Y=1\\mid X=1\\right) & =P\\left(Y=0\\mid X=0\\right)=1-\\epsilon\n",
    "\\end{align*}\n",
    "The law of total probability allows to write \n",
    "\\begin{align*}\n",
    "P\\left(Y=1\\right) & =P\\left(X=0\\right)P\\left(Y=1\\mid X=0\\right)+P\\left(X=1\\right)P\\left(Y=1\\mid X=1\\right)=p\\epsilon+\\left(1-p\\right)\\left(1-\\epsilon\\right)\\\\\n",
    "P\\left(Y=0\\right) & =P\\left(X=0\\right)P\\left(Y=0\\mid X=0\\right)+P\\left(X=1\\right)P\\left(Y=0\\mid X=1\\right)=p\\left(1-\\epsilon\\right)+\\left(1-p\\right)\\epsilon\n",
    "\\end{align*}\n",
    "Therefore, $Y\\sim\\Ber_{\\theta}$ where $\\theta=p\\epsilon+\\left(1-p\\right)\\left(1-\\epsilon\\right)$.\n",
    "\n",
    "The bit error is represented by the Borel set (in the 2-D plane)\n",
    "\\\\[\n",
    "\\mathsf{B}=\\left\\{ \\left(0,1\\right),\\left(1,0\\right)\\right\\} \n",
    "\\\\]\n",
    "with pre-image\n",
    "\\\\[\n",
    "\\mathsf{A}=\\left(Y=1,X=0\\right)\\cup\\left(Y=0,X=1\\right)\n",
    "\\\\]\n",
    "and the resulting bit-error probability \n",
    "\\\\[\n",
    "P\\left(\\mathsf{A}\\right)=P\\left(Y=1,X=0\\right)+P\\left(Y=0,X=1\\right)=p\\epsilon+\\left(1-p\\right)\\epsilon=\\epsilon\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "graffitiCellId": "id_jg2qy00"
   },
   "source": [
    "#### Typical value of a distribution\n",
    "Consider a discrete distribution with values $k\\in\\mathcal{X}^{\\#}$ and probabilities $p_{k}$. If the distribution is symmetrical about a single peak, then the most likely value, called the mode, can be considered a typical value. For distributions of other shapes, the mode may not be appropriate. Other alternatives include the median and the mean. The median $m$ of a distribution generalizes the notion of middle value and is defined as the value at which the CDF equals $1/2$. The mean $\\mu$ of a distribution is the value that best fits long-run observations in a least squared sense.\n",
    "\n",
    "<img src=\"images/chance_regularity_t-plot.png\" width=\"50%\" />\n",
    "\n",
    "We can get an estimate $\\hat{\\mu}_{n}$ of the mean based on the sum of its squared Euclidean distances from observations in $n$ trials of the underlying experiment, treated as a function of $\\hat{\\mu}_{n}$,\n",
    "$$\n",
    "S\\left(\\hat{\\mu}_{n}\\right)=\\sum_{k\\in\\mathcal{X}^{\\#}}\\left(k-\\hat{\\mu}_{n}\\right)^{2}a_{n}\\left(k\\right)\n",
    "$$\n",
    "where $a_{n}\\left(k\\right)$ is the observed frequency of $k$ in $n$ trials. Note that absolute frequencies must add up to $n$. Since $\\hat{\\mu}_{n}$ minimizes $S\\left(\\hat{\\mu}_{n}\\right)$ by definition, we require $S^{\\prime}\\left(\\hat{\\mu}_{n}\\right)=0$ which leads to\n",
    "$$\n",
    "-2\\sum_{k\\in\\mathcal{X}^{\\#}}\\left(k-\\hat{\\mu}_{n}\\right)a_{n}\\left(k\\right)=0\\implies n\\hat{\\mu}_{n}=\\sum_{k\\in\\mathcal{X}^{\\#}}ka_{n}\\left(k\\right)\\implies\\hat{\\mu}_{n}=\\frac{1}{n}\\sum_{k\\in\\mathcal{X}^{\\#}}ka_{n}\\left(k\\right)=\\sum_{k\\in\\mathcal{X}^{\\#}}kr_{n}\\left(k\\right)\n",
    "$$\n",
    "where $r_{k}\\left(n\\right)$ is the relative frequency of $k$ in $n$ trials. Since the long-run relative frequencies $r_{n}\\left(k\\right)$ approach probabilities $p_{k}$, we have the following formula for the mean of a discrete distribution\n",
    "$$\n",
    "\\mu=\\sum_{k\\in\\mathcal{X}^{\\#}}kp_{k}\n",
    "$$\n",
    "This makes sense as each value $x_{i}$ of the RV $X$ is weighted by the associated probability $p_{i}$. Owing to this probability weighting, the mean is also called the expected value, or expectation, of the RV. The expectation of a RV $X$, denoted by $\\EE X$, is defined by\n",
    "$$\n",
    "\\mu_{X}=\\EE X=\\intop_{\\mathbb{R}}xP_{X}\\left(\\diff \\mathsf{B}_{x}\\right)=\\sum_{k\\in\\mathcal{X}^{\\#}}kf^{\\#}_{X}\\left(k\\right)+\\intop_{\\mathcal{X}}xf^{\\wedge}_{X}\\left(x\\right)\\diff x\n",
    "$$\n",
    "Note that the expectation is not a point function like a transformation. Instead, the expectation is an example of what is called an operator. An operator takes a function and returns another function. A special kind of an operator, called a functional, collapses a function to a number. You can now see that the expectation operator acts on a RV, a function, and returns the mean, a number. Thus, the expectation operator is a functional. The notation $\\EE X$ without using any brackets not only reminds of the operator involved but also helps to distinguish it from a transformation $g\\left(X\\right)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Let us work out the mean for the two RVs discussed above.\n",
    "\n",
    ">**Dirac distribution**\n",
    ">The mean of the deterministic RV $a\\sim\\delta_{a}$ is\n",
    "$$\n",
    "\\EE a=\\sum_{k\\in\\left\\{ a\\right\\} }k\\delta_{a}\\left(\\left\\{ k\\right\\} \\right)=a\n",
    "$$\n",
    "In other words, the expected value of a (deterministic) constant is the constant itself.\n",
    "\n",
    ">**Bernoulli distribution**\n",
    ">The mean of the Bernoulli RV $X\\sim\\Ber_{p}$ is\n",
    "$$\n",
    "\\EE X=\\sum_{k=0}^{1}k\\mathrm{Ber}\\left(k;p\\right)=\\sum_{k=0}^{1}kp^{k}\\left(1-p\\right)^{1-k}=p\n",
    "$$\n",
    "\n",
    ">**Discrete uniform distribution (standard)**\n",
    ">The mean of the discrete uniform RV $X\\sim\\Ud_{n}$ is\n",
    "\\\\[\n",
    "\\mu_{X}=\\EE X=\\sum_{k=1}^{n}k\\Ud\\left(k;n\\right)=\\frac{1}{n}\\sum_{k=1}^{n}k=\\frac{n+1}{2}\n",
    "\\\\]\n",
    ">**Mean number of dots facing up on two dice**\n",
    "Recall the throw of two dice and the RV $X$ representing the number\n",
    "of dots facing up on both the dice. \n",
    "\\begin{align*}\n",
    "\\mu_{X}=\\EE X & =\\sum_{k=2}^{12}kp_{k}\\\\\n",
    " & =\\sum_{k=2}^{7}k\\left(\\frac{k-1}{36}\\right)+\\sum_{k=8}^{12}k\\left(\\frac{13-k}{36}\\right)\\\\\n",
    " & =\\frac{1}{36}\\left[\\sum_{k=2}^{7}k\\left(k-1\\right)+12\\sum_{k=8}^{12}k-\\sum_{k=8}^{12}k\\left(k-1\\right)\\right]\\\\\n",
    " & =\\frac{1}{36}\\left[2\\sum_{k=2}^{7}k\\left(k-1\\right)+12\\left(\\sum_{k=1}^{12}k-\\sum_{k=1}^{7}k\\right)-\\sum_{k=2}^{12}k\\left(k-1\\right)\\right]\n",
    "\\end{align*}\n",
    "These seemingly intractable sums can be reduced by employing what is called the forward difference operator. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Forward difference operator\n",
    "The forward difference operator, denoted $\\Delta$, is defined by\n",
    "its action on a sequence, namely\n",
    "\\\\[\n",
    "\\Delta s_{k}=s_{k+1}-s_{k}\n",
    "\\\\]\n",
    "Essentially, it is a shift operator. We will now investigate the behavior\n",
    "of the relevant polynomials under $\\Delta$. \n",
    "\n",
    "1. Quadratic falling factorial\n",
    "\\\\[\n",
    "\\Delta\\left(k\\right)_{2}=\\Delta\\left[k\\left(k-1\\right)\\right]=\\left(k+1\\right)k-k\\left(k-1\\right)=2k\n",
    "\\\\]\n",
    "which can be exploited to get a formula for the integer sum \n",
    "\\\\[\n",
    "\\sum_{k=1}^{n}k=\\frac{1}{2}\\sum_{k=1}^{n}\\Delta\\left(k\\right)_{2}=\\frac{\\left(n+1\\right)_{2}-\\left(1\\right)_{2}}{2}=\\frac{\\left(n+1\\right)n}{2}\n",
    "\\\\]\n",
    "1. Cubic falling factorial\n",
    "\\\\[\n",
    "\\Delta\\left(k\\right)_{3}=\\Delta\\left[k\\left(k-1\\right)\\left(k-2\\right)\\right]=\\left(k+1\\right)k\\left(k-1\\right)-k\\left(k-1\\right)\\left(k-2\\right)=3k\\left(k-1\\right)\n",
    "\\\\]\n",
    "which can be exploited to get another useful sum \n",
    "\\\\[\n",
    "\\sum_{k=2}^{n}k\\left(k-1\\right)=\\frac{1}{3}\\sum_{k=2}^{n}\\Delta\\left(k\\right)_{3}=\\frac{\\left(n+1\\right)_{3}-\\left(2\\right)_{3}}{3}=\\frac{n\\left(n^{2}-1\\right)}{3}\n",
    "\\\\]\n",
    "We will use these identities to evaluate the sums in the foregoing expectation. \n",
    ">**Mean number of dots facing up on two dice (continued)**\n",
    ">The integer sums:\n",
    "\\begin{align*}\n",
    "\\sum_{k=1}^{12}k & =\\frac{13\\times 12}{2}=78\\\\\n",
    "\\sum_{k=1}^{7}k & =\\frac{8\\times 7}{2}=28\\\\\n",
    "\\sum_{k=8}^{12}k & =78-28=50\n",
    "\\end{align*}\n",
    "The quadratic falling factorial sums\n",
    "\\begin{align*}\n",
    "\\sum_{k=2}^{12}k\\left(k-1\\right) & =\\frac{12\\left(12^{2}-1\\right)}{3}=572\\\\\n",
    "\\sum_{k=2}^{7}k\\left(k-1\\right) & =\\frac{7\\left(7^{2}-1\\right)}{3}=112\\\\\n",
    "\\sum_{k=8}^{12}k\\left(k-1\\right) & =572-112=460\n",
    "\\end{align*}\n",
    "Putting it all together yields the required expectation in the twice\n",
    "dice-throw problem,\n",
    "\\\\[\n",
    "\\mu_{X}=\\EE X=\\frac{1}{36}\\left[112+12\\times 50-460\\right]=7\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "graffitiCellId": "id_ou7x9gu"
   },
   "source": [
    "#### Expectation of a transformed RV\n",
    "Consider a transformation $Y=g\\left(X\\right)$ where $g$ is a point function making any value $x$ of $X$ to the value $g\\left(x\\right)$ of $Y$. How are the two expectations $\\EE X$ and $\\EE Y$ related? Let us work that out for the easier case of discrete distribution first. Any value $x$ of $X$ determines the value $y$ of $Y$ according\n",
    "to $y=g\\left(x\\right)$. Since both the RVs are defined on the same probability space, the following correspondence between events is evident\n",
    "\\\\[\n",
    "\\left(Y=k\\right)=\\left(g\\left(X\\right)=k\\right)=\\bigcup_{g\\left(i\\right)=k}\\left(X=i\\right)\n",
    "\\\\]\n",
    "The expectation of $Y$ then follows from the law of total probability:\n",
    "\\\\[\n",
    "\\EE Y=\\sum_{k\\in\\mathcal{Y}}kf_{Y}\\left(k\\right)=\\sum_{k\\in\\mathcal{Y}}kP\\left(Y=k\\right)=\\sum_{k\\in\\mathcal{Y}}k\\sum_{g\\left(i\\right)=k}P\\left(X=i\\right)=\\sum_{i\\in\\mathcal{X}}g\\left(i\\right)f_{X}\\left(k\\right)\n",
    "\\\\]\n",
    "In other words, we do not need the distribution of $Y$ to compute the expectation. This makes sense in terms of long-run relative frequencies: the observed long-run frequency of a value $x$ is also the frequency of $g\\left(x\\right)$.\n",
    "\n",
    "This result can be extended to general RVs. The pre-image of any Borel\n",
    "set $\\mathsf{B}$ can be expressed as a disjoint union\n",
    "\\\\[\n",
    "Y^{-1}\\left[\\diff\\mathsf{B}_{y}\\right]=\\bigcup_{g\\left(x\\right)=y}X^{-1}\\left[\\diff\\mathsf{B}_{x}\\right]\n",
    "\\\\]\n",
    "The expectation of $Y$ then follows from the law of total probability:\n",
    "\\\\[\n",
    "\\EE Y=\\intop_{\\mathbb{R}}yP_{Y}\\left(\\diff\\mathsf{B}_{y}\\right)=\\intop_{\\mathbb{R}}y\\sum_{g\\left(x\\right)=y}P_{X}\\left(\\diff\\mathsf{B}_{x}\\right)=\\intop_{\\mathbb{R}}g\\left(x\\right)P_{X}\\left(\\diff\\mathsf{B}_{x}\\right)\n",
    "\\\\]\n",
    "This leads to a more general definition of the expectation:\n",
    "\\\\[\n",
    "\\EE g\\left(X\\right)=\\intop_{\\mathbb{R}}g\\left(x\\right)P_{X}\\left(\\diff\\mathsf{B}_{x}\\right)\n",
    "\\\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "graffitiCellId": "id_6d97bnd"
   },
   "source": [
    "#### Linearity of the expectation (simpler)\n",
    "Recall from math and circuit courses that integration and summation are linear operations. You can verify from the above definition that the expectation inherits linearity from integration and summation in the following sense:\n",
    "$$\n",
    "\\EE \\left(aX+b\\right)=a\\EE X+\\EE b=a\\EE X+b\n",
    "$$\n",
    "Setting $a=1$ and $b=-\\mu_{X}$ gives \n",
    "$$\n",
    "\\EE \\left(X-\\mu_{X}\\right)=\\EE X-\\mu_{X}=0\n",
    "$$\n",
    "as expected.\n",
    "\n",
    "Two examples can be worked out here.\n",
    ">**The two-point distribution**\n",
    ">Since we know the mean of the Bernoulli RV $X\\sim\\Ber_{p}$ is $p$, the mean of the transformed (two-point) RV $Y=a+\\left(b-a\\right)X$ follows from linearity\n",
    "\\\\[\n",
    "\\EE Y=a+\\left(b-a\\right)\\EE X=a+\\left(b-a\\right)p=\\left(1-p\\right)a+pb\n",
    "\\\\]\n",
    ">**The discrete uniform distribution (transformed)**\n",
    ">Since we know the mean of the standard discrete uniform RV $X\\sim\\Ud_{n}$, the mean of the transformed RV $Y=a-1+X$ follows from linearity\n",
    "\\\\[\n",
    "\\EE Y=a-1+\\EE X=a-1+\\left.\\frac{n+1}{2}\\right|_{n=b-a+1}=a-1+\\frac{b-a+2}{2}=\\frac{a+b}{2}\n",
    "\\\\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Linearity of the expectation (detailed)\n",
    "A natural question arises in a curious mind here: why did we not try to define linearity of the expectation by this equation\n",
    "\\\\[\n",
    "\\EE\\left(aX+bY\\right)=a\\EE X+b\\EE Y\n",
    "\\\\]\n",
    "which is very reasonable to expect. To see the complication in this seemingly innocent equation, assume $X$ and $Y$ to be discrete, for convenience, and define a new RV $Z=aX+bY$ as a linear transformation. Any pair $\\left(x,y\\right)$ of values of $\\left(X,Y\\right)$ determines the value $z$ of $Z$ according to $z=ax+by$. Since all the three RVs are defined on the same probability space, the following correspondence between events is evident: \n",
    "\\\\[\n",
    "\\left(Z=k\\right)=\\left(aX+bY=k\\right)=\\bigcup_{ai+bj=k}\\left(X=i\\cap Y=j\\right)\n",
    "\\\\]\n",
    "The expectation of $Z$ then follows from the law of total probability:\n",
    "\\begin{align*}\n",
    "\\EE Z=\\sum_{k\\in\\mathcal{Z}}kP\\left(Z=k\\right) & =\\sum_{k\\in\\mathcal{Z}}k\\sum_{ai+bj=k}P\\left(X=i\\cap Y=j\\right)\\\\\n",
    " & =\\sum_{i\\in\\mathcal{X}}\\sum_{j\\in\\mathcal{Y}}\\left(ai+bj\\right)P_{X,Y}\\left(\\left\\{ i\\right\\} \\times\\left\\{ j\\right\\} \\right)\\\\\n",
    " & =a\\sum_{i\\in\\mathcal{X}}i\\sum_{j\\in\\mathcal{Y}}P_{X,Y}\\left(\\left\\{ i\\right\\} \\times\\left\\{ j\\right\\} \\right)+b\\sum_{j\\in\\mathcal{Y}}j\\sum_{i\\in\\mathcal{X}}P_{X,Y}\\left(\\left\\{ i\\right\\} \\times\\left\\{ j\\right\\} \\right)\\\\\n",
    " & =a\\sum_{i\\in\\mathcal{X}}iP_{X}\\left(\\left\\{ i\\right\\} \\right)+b\\sum_{j\\in\\mathcal{Y}}jP_{Y}\\left(\\left\\{ j\\right\\} \\right)\\\\\n",
    " & =a\\EE X+b\\EE Y\n",
    "\\end{align*}\n",
    "This result can be extended to general RVs. The pre-image of any Borel set $\\mathsf{B}$ of the $z$-axis can be expressed as a disjoint\n",
    "union\n",
    "\\\\[\n",
    "Z^{-1}\\left[\\diff \\mathsf{B}_{z}\\right]=\\bigcup_{ax+by=z}X^{-1}\\left[\\diff \\mathsf{B}_{x}\\right]\\cap Y^{-1}\\left[\\diff \\mathsf{B}_{y}\\right]\n",
    "\\\\]\n",
    "The expectation of $Z$ then follows from the law of total probability:\n",
    "\\begin{align*}\n",
    "\\EE Z=\\intop_{\\mathbb{R}}zP_{Z}\\left(\\diff \\mathsf{B}_{z}\\right) & =\\intop_{\\mathbb{R}}z\\intop_{ax+by=z}P_{X,Y}\\left(\\diff \\mathsf{B}_{x}\\times\\diff \\mathsf{B}_{y}\\right)\\\\\n",
    " & =\\intop_{\\mathbb{R}}\\intop_{\\mathbb{R}}\\left(ax+by\\right)P_{X,Y}\\left(\\diff \\mathsf{B}_{x}\\times\\diff \\mathsf{B}_{y}\\right)\\\\\n",
    " & =a\\intop_{\\mathbb{R}}x\\intop_{\\mathbb{R}}P_{X,Y}\\left(\\diff \\mathsf{B}_{x}\\times\\diff \\mathsf{B}_{y}\\right)+b\\intop_{\\mathbb{R}}y\\intop_{\\mathbb{R}}P_{X,Y}\\left(\\diff \\mathsf{B}_{x}\\times\\diff \\mathsf{B}_{y}\\right)\\\\\n",
    " & =a\\intop_{\\mathbb{R}}xP_{X}\\left(\\diff \\mathsf{B}_{x}\\right)+b\\intop_{\\mathbb{R}}yP_{Y}\\left(\\diff \\mathsf{B}_{y}\\right)\\\\\n",
    " & =a\\EE X+b\\EE Y\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false",
    "graffitiCellId": "id_pb82llw"
   },
   "source": [
    "#### Spread of a distribution\n",
    "A measure of spread of a distribution is the so-called standard deviation, denoted $\\sigma$, which can be viewed as the Euclidean distance between a typical pair of values in a distribution. The mean was introduced as a typical value in the sense of minimizing the sum of its squared Euclidean distances from long-run observations. That gives us a hint of an estimate $\\hat{\\sigma}_{n}^{2}$ of the squared standard deviation--the average squared distance per trial, \n",
    "$$\n",
    "\\hat{\\sigma}_{n}^{2}=\\frac{1}{n}S\\left(\\hat{\\mu}_{n}\\right)=\\frac{1}{n}\\sum_{k\\in\\mathcal{X}^{\\#}}\\left(k-\\hat{\\mu}_{n}\\right)^{2}a_{n}\\left(k\\right)=\\sum_{k\\in\\mathcal{X}^{\\#}}\\left(k-\\hat{\\mu}_{n}\\right)^{2}r_{n}\\left(k\\right)\n",
    "$$\n",
    "To see why $\\hat{\\sigma}_{n}$ is the Euclidean distance between a\n",
    "typical pair of values, note that both $\\left(\\hat{\\mu}_{n},\\hat{\\mu}_{n}+\\hat{\\sigma}_{n}\\right)$\n",
    "and $\\left(\\hat{\\mu}_{n},\\hat{\\mu}_{n}-\\hat{\\sigma}_{n}\\right)$ can\n",
    "be viewed as typical pairs because\n",
    "\\\\[\n",
    "\\left(\\hat{\\mu}_{n}\\pm\\hat{\\sigma}_{n}-\\hat{\\mu}_{n}\\right)^{2}=\\hat{\\sigma}_{n}^{2}\n",
    "\\\\]\n",
    "That leads us to define the standard deviation $\\sigma$ of a discrete distribution as the positive square root of what is called the variance of the distribution,\n",
    "$$\n",
    "\\sigma^{2}=\\sum_{k\\in\\mathcal{X}^{\\#}}\\left(k-\\hat{\\mu}\\right)^{2}p_{k}\n",
    "$$\n",
    "The variance of a general RV $X$, denoted $\\Var X$,\n",
    "is defined by\n",
    "$$\n",
    "\\sigma_{X}^{2}=\\Var X=\\EE \\left(X-\\mu_{X}\\right)^{2}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Nonlinearity of variance (simpler)\n",
    "The squared deviation in the above formula prevents the variance operator\n",
    "to inherit linearity from integration and summation. We can study\n",
    "the nature of nonlinearity of variance by considering a linear transformation\n",
    "$Y=aX+b$ of a RV $X$ whose variance is known. Correspondence between\n",
    "values and means take the form\n",
    "\\\\[\n",
    "y=ax+b,\\quad\\mu_{Y}=a\\mu_{X}+b\n",
    "\\\\]\n",
    "This all leads to \n",
    "\\\\[\n",
    "\\Var Y=\\EE\\left(Y-\\mu_{Y}\\right)^{2}=\\EE\\left(aX+b-a\\mu_{X}-b\\right)^{2}=\\EE a^{2}\\left(X-\\mu_{X}\\right)^{2}=a^{2}\\Var X\n",
    "\\\\]\n",
    "As we expected, $\\Var\\left(aX+b\\right)\\ne a\\Var X+b$ thanks to the\n",
    "nonlinearity of variance.\n",
    "\n",
    "### Nonlinearity of variance (detailed)\n",
    "Consider a linear transformation $Z=aX+bY$ of two RVs with known\n",
    "variances. Correspondence between values and means take the form\n",
    "\\\\[\n",
    "z=ax+by,\\quad\\mu_{Z}=a\\mu_{X}+b\\mu_{Y}\n",
    "\\\\]\n",
    "This all leads to \n",
    "\\begin{align*}\n",
    "\\Var Z=\\EE\\left(Z-\\mu_{Z}\\right)^{2} & =\\EE\\left(aX+bY-a\\mu_{X}-b\\mu_{Y}\\right)^{2}\\\\\n",
    " & =a^{2}\\EE\\left(X-\\mu_{X}\\right)^{2}+b^{2}\\EE\\left(Y-\\mu_{Y}\\right)^{2}+ab\\EE\\left(X-\\mu_{X}\\right)\\left(Y-\\mu_{Y}\\right)\\\\\n",
    " & =a^{2}\\Var X+b^{2}\\Var Y+ab\\EE\\left(X-\\mu_{X}\\right)\\left(Y-\\mu_{Y}\\right)\n",
    "\\end{align*}\n",
    "As we expected, $\\Var\\left(aX+bY\\right)\\ne a\\Var X+b\\Var Y$ thanks\n",
    "to the nonlinearity of variance. The last term in the above equation\n",
    "involves what is called the covariance between two RVs:\n",
    "\\\\[\n",
    "\\Cov\\left(X,Y\\right)=\\EE\\left(X-\\mu_{X}\\right)\\left(Y-\\mu_{Y}\\right)\n",
    "\\\\]\n",
    "Let us work out the variance for the two RVs discussed above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
